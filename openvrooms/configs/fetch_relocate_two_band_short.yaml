# scene
scene: relocate
scene_id: scene0420_01_multi_band
load_texture: true
obj_num: 1 #2
duplicated_objects: true

# robot
robot: Fetch
agent_initial_pos: [-1.8, -0.9, 0] #[-1.6, -1, 0]
agent_initial_orn: [0, 0, 0] #[0, 0, 0.64]
body_width: 0.5  # diameter of base link

# action space
is_discrete: true #false
wheel_velocity: 0.5 # normalized wheel velocity
move_distance: 0.1  #0.05
turn_angle: 0.1 #0.03
joint_control: true

# objects: pos:[x,y], orn: eular angles
obj_initial_pos: [[-1.2, -0.9]] #[[-0.5, -0.5]] 
obj_initial_orn: [[0, 0, 0]]  # [[0, 0, 0.64]]
obj_target_pos: [[1.2, 0.9]] #[[1.5, 1]] 
obj_target_orn: [[0, 0, 0]] # [[0, 0, 0.64]]
obj_mass: [10]
floor_friction: [0.2, 0.2, 0.8]
floor_borders: [-1, 0.] #[-1., 0.]
border_type: "y_border"

# observation space: sensor spec
output: [task_obs]  # if learning, can only handle single modal  #task_obs or rgb
third_person_view: true
external_camera_pos: [2, -0.3, 2.3] #[2, 0, 2.3] #[1, 0, 2.3] 
external_camera_view_direction: [-0.4, 0.0, -0.4] #[-0.4, 0.3, -0.9] #[-0.3, 0, -0.8] #[-0.5, 0, -0.6] 

#external_camera_pos: [1.79087593, -1.50272334, 1.88004946]
#external_camera_view_direction: [-0.69855025, 0.58615326, -0.9]
image_width: 84
image_height: 84

# task
task: relocate_goal_fixed
goal_format: cartesian
task_obs_dim: 6 # basic dimensions in state, addtional dimensions 6 per object

# reward
use_goal_dist_reward: false
rot_dist_reward_weight: 0.0 # (position distance reward weight is 1)
goal_dist_reward_weight: 0.1 #1.0 # should > 0

success_reward: 100.0 #800.0
dist_tol: 0.25 #0.1 # l2 distance in meter
angle_tol: 0.2 # eular angle distance ?

collision_penalty: -10.0
collision_reward: -0.5 #1.0 #0.2 #-0.1
time_elapse_reward: -1.0 #0.0
out_of_bound_reward: -1000.0

# use energy in reward function
use_energy_cost: false
joint_level_energy: false # false: pushing energy, true: robot output energy
normalized_energy: false #false  # for joint level energy only
ratio_method: "history" # "paper" "history" "heuristic"
heuristic_succeed_episode_energy_min: 30 # only for ratio_method is heuristic
heuristic_succeed_episode_energy_max: 250 #380 # only for ratio_method is heuristic

# episode termination condition
max_step: 400
max_collisions_allowed: 100

# misc config
collision_ignore_link_a_ids: [0, 1, 2]  # ignore collisions with these robot links
visual_object_visible_to_agent: false # visual target landmarks

# simulation
action_timestep: 0.1  # environment executes action per action_timestep second, 1/10
#physics_timestep: 0.025 # physics timestep for pybullet, 1/40
initial_pos_z_offset: 0.1

#-----------------------------------------------------------------------------------------
# rl for ALL
# general training settings
# training_timesteps: 40000
# discount_factor: 0.99
# learning_rate: 0.001
# minibatch_size: 64
# frame_stack: 4

# # dqn
# update_frequency: 1
# target_update_frequency: 100
# replay_start_size: 1000
# replay_buffer_size: 10000
# initial_exploration: 1.0
# final_exploration: 0.0
# final_exploration_frame: 10000




